{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c6e2e0e-5764-4cfe-9c72-a1c503df38ad",
   "metadata": {},
   "source": [
    "# COCO Instance Segmentation\n",
    "\n",
    "COCO is a large image dataset designed for object detection, segmentation, person keypoints detection, stuff segmentation, and caption generation. We will focus on instance segmentation, i.e. distinguishing each category, giving different labels for individual instances in the same type of objects. It can be regarded as delivering the tasks of object detection and semantic segmentation at the same time. We will be using GPU of Hyperion server of City, University of London. Data is pre-downloaded from https://www.kaggle.com/datasets/awsaf49/coco-2017-dataset to Hyperion under directory '/mnt/data/public/coco2017'. Data original source: https://cocodataset.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef634c21-680a-4387-92c7-66b21cb7b860",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycocotools\n",
    "from pycocotools.coco import COCO\n",
    "import cython\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import torch, torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.ops.boxes import box_convert\n",
    "\n",
    "### For visualizing the outputs ###\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "   device = torch.device('cuda')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a5ea75-42c7-47d6-882d-c232ab8c399d",
   "metadata": {},
   "source": [
    "## Check Dataset File Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf717f1-37d5-421f-ac9e-8c4aa473085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/mnt/data/public/coco2017/coco2017'\n",
    "\n",
    "files = os.listdir(path)\n",
    "\n",
    "for f in files:\n",
    "\tprint(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d7d5d9-085d-4c60-a151-dbbbec91181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_a = path + '/annotations'\n",
    "\n",
    "files = os.listdir(path_a)\n",
    "\n",
    "for f in files:\n",
    "\tprint(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d123b5-dbec-4994-9594-1a18388a9079",
   "metadata": {},
   "source": [
    "## Check Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50e385c-7f1d-43b0-b087-a12b53a27fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir='/mnt/data/public/coco2017/coco2017/annotations'\n",
    "dataType='val'\n",
    "annFile='{}/instances_{}2017.json'.format(dataDir,dataType)\n",
    "\n",
    "# Initialize the COCO api for instance annotations\n",
    "coco_val=COCO(annFile)\n",
    "\n",
    "# Load the categories in a variable\n",
    "catIDs = coco_val.getCatIds()\n",
    "cats = coco_val.loadCats(catIDs)\n",
    "\n",
    "print(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef65750-7d84-40d0-aab1-79badf4db618",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_file='/mnt/data/public/coco2017/coco2017/annotations/instances_train2017.json' #file of coco dataset annotations \n",
    "\n",
    "# Initialize the COCO api for instance annotations\n",
    "coco_train=COCO(annotations_file)\n",
    "\n",
    "# Load the categories in a variable\n",
    "catIDs = coco_train.getCatIds()\n",
    "cats = coco_train.loadCats(catIDs)\n",
    "\n",
    "print(cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46501e64-77ba-48da-8a41-f32d3618840b",
   "metadata": {},
   "source": [
    "We can see that the category id of person is 1.\n",
    "\n",
    "Let's see the number of images for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd2e4b3-1478-4a58-9d9e-0544d6513baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(80):\n",
    "    print('{} - category id: {}, count of training images: {}'.format(cats[i]['name'], cats[i]['id'], len(coco_train.getImgIds(catIds=[i]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b56d25-5df0-4ba3-a92e-59148982da14",
   "metadata": {},
   "source": [
    "# Mask R-CNN\n",
    "The code for mask R-CNN uses torchvision model, and is largely based on official tutorial from pytorch.org on finetuning mask R-CNN, with customization to the server environment (i.e. its absence of direct access to Internet) and our use case.\n",
    "\n",
    "Reference:\n",
    "https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html and Lecture notes, INM705 DEEP LEARNING FOR IMAGE ANALYSIS - Lab5 by Dr Alex Ter-Sarkisov@City, University of London\n",
    "\n",
    "From https://github.com/pytorch/vision.git, pre-download engine.py, utlis.py, transform.py (and they depend on coco_eval.py, coco_utlis.py, so they also need to be downloaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260213af-d5c9-4986-815f-e1e0f5210e42",
   "metadata": {},
   "source": [
    "Let's see one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e48d96d-f0ae-407f-a731-758a0b2dc376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.open(path+'/test2017/000000112691.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc56bb8-d432-418b-a171-d0bad5a64292",
   "metadata": {},
   "source": [
    "# Finetuning from a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e810613f-abdf-4ece-9a2a-0557eabdb7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# load a model pre-trained pre-trained on COCO; need to download weights manually beforehand as Hyperion is not connected to internet directly\n",
    "# Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\"\n",
    "# Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\"\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False,)\n",
    "pretrained_weights = torch.load('fasterrcnn_resnet50_fpn_coco-258fb6c6.pth', map_location='cpu')\n",
    "# # copy only backbone weights\n",
    "for _n, _par in model.state_dict().items():\n",
    "     if 'backbone' in _n:\n",
    "        _par.requires_grad = False\n",
    "        _par.copy_(pretrained_weights[_n])\n",
    "        _par.requires_grad = True\n",
    "\n",
    "if device == torch.device('cuda'):\n",
    "    model = model.to(device)\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (person) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1a823a-d593-41d5-b142-d39043377e4e",
   "metadata": {},
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1214a628-c423-4d5a-96ac-d33206038268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import python modules downloaded from pytorch github https://github.com/pytorch/vision/tree/main/references/detection  \n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eead85-ab6e-41ea-a85d-b2520c956542",
   "metadata": {},
   "source": [
    "## Explore annotations\n",
    "\n",
    "Lets's explore the annotations and data structure a bit before creating the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91466aa-919f-4a1f-b767-0b2520b32982",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnnIds = coco_train.getAnnIds(catIds=[1], areaRng=[], iscrowd=False)\n",
    "anns_obj1 = coco_train.loadAnns(AnnIds)\n",
    "print(anns_obj1[0])\n",
    "anns_obj2 = [ann for ann in anns_obj1 if len(coco_train.annToMask(ann)) > 0]  #remove empty mask\n",
    "print(anns_obj2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb1324d-c731-4cc5-9483-ef251ba8f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(anns_obj1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22919bf7-2981-4184-8bb1-b8242bfbcde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(anns_obj2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a460df-6b15-4c8a-af5e-a5a28ae2f2a8",
   "metadata": {},
   "source": [
    "It shows that it does not have empty mask for non-crowd person category. Let's see the annotations and image object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d884d811-715b-4b8c-ab7b-d5254dd27324",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coco_train.annToMask(anns_obj1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e643b79-bb5e-41fd-939f-38ed4bdb1599",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = anns_obj1[0]['id']\n",
    "img_obj = coco_train.loadImgs(image_id)[0]\n",
    "print(img_obj['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6feacf-95f5-4cad-ab86-ab1197f105f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_obj1[0]['bbox'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf5edae-273e-4448-8613-9c3f51f7df57",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4352d60-83a9-43f3-9066-aa3f83b2a329",
   "metadata": {},
   "source": [
    "# Define COCO dataset\n",
    "\n",
    "While there are online examples of dataset codes, we need to customize to our needs by setting category to focus, excluding crowd for better data quality, and cleaning the data by removing those without mask annotations.\n",
    "\n",
    "One note on the labels. The model considers class 0 as background. So we should avoid labeling any class as 0 if we don't have background. \n",
    "\n",
    "Let's focus on bird and giraffe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3009f03a-3b82-448c-9005-a48087c99316",
   "metadata": {},
   "source": [
    "Let's deine COCO dataset class for dataloader of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695e15a1-c7f5-4efc-a052-ceb6c3e9a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspired and adapt from reference below, with customization and correction\n",
    "#reference: https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "           #https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/coco.py\n",
    "           #https://stackoverflow.com/questions/68513782/use-ms-coco-format-as-input-to-pytorch-maskrcnn\n",
    "           #Lecture notes, INM705 DEEP LEARNING FOR IMAGE ANALYSIS - Lab5 by Dr. Alex Ter-Sarkisov @ City, University of London\n",
    "\n",
    "dataset_dir='/mnt/data/public/coco2017/coco2017'\n",
    "focus_category = [16, 25]   #16: bird category; 25: giraffe\n",
    "\n",
    "class CocoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_dir, subset='train', transforms=None, focus_category=focus_category, areaRng=[], iscrowd=False):\n",
    "        ann_file = '{}/annotations/instances_{}2017.json'.format(dataset_dir, subset)\n",
    "        self.imgs_dir = os.path.join(dataset_dir, subset+'2017')\n",
    "        self.coco = COCO(ann_file)\n",
    "        AnnIds = self.coco.getAnnIds(catIds=focus_category, areaRng=areaRng, iscrowd=iscrowd)\n",
    "        self.anns_obj = self.coco.loadAnns(AnnIds)\n",
    "        self.image_id_all = [ ann['image_id'] for ann in self.anns_obj ]\n",
    "        self.image_id_all = np.unique(np.array(self.image_id_all, dtype=int))\n",
    "        self.catlabel = {}\n",
    "        for i in range(len(focus_category)):\n",
    "            self.catlabel[focus_category[i]] = i +1   #note: lable 0 reserved for background category in the model\n",
    "        self.transforms = transforms\n",
    "        self.focus_category = focus_category\n",
    "        self.areaRng = areaRng\n",
    "        self.iscrowd = iscrowd\n",
    "        \n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        self.length = len(self.image_id_all)\n",
    "        return self.length  \n",
    "\n",
    "    def mabic(self, ann, masks, areas, boxes, image_id_instance, catIds):\n",
    "        #print('ann type: {}; ann: {}'.format(type(ann),ann))\n",
    "        #print('ann area: {}'.format(ann['area']))\n",
    "        areas.append(ann['area'])\n",
    "        masks.append(self.coco.annToMask(ann))\n",
    "        #1e-7 solves float num rounding issue\n",
    "        boxes.append([ann['bbox'][0], ann['bbox'][1], ann['bbox'][0]+ann['bbox'][2]+1e-7, ann['bbox'][1]+ann['bbox'][3]+1e-7]) \n",
    "        image_id_instance.append(ann['image_id'])\n",
    "        catIds.append(ann['category_id'])\n",
    "        return masks, areas, boxes, image_id_instance, catIds\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Args:\n",
    "            idx: index of sample to be fed\n",
    "        return:\n",
    "            dict containing:\n",
    "            - PIL Image of shape (H, W)\n",
    "            - target (dict) containing: \n",
    "                - boxes:    FloatTensor[N, 4], N being the nÂ° of instances and it's bounding \n",
    "                boxe coordinates in [x0, y0, x1, y1] format, ranging from 0 to W and 0 to H;\n",
    "                - labels:   Int64Tensor[N], class label (0 is background);\n",
    "                - image_id: Int64Tensor[1], unique id for each image;\n",
    "                - area:     Tensor[N], area of bbox;\n",
    "                - iscrowd:  UInt8Tensor[N], True or False;\n",
    "                - masks:    UInt8Tensor[N, H, W], segmantation maps;\n",
    "        '''\n",
    "    \n",
    "        #get image id and then get annotation object\n",
    "        image_id = self.image_id_all[idx]\n",
    "        AnnIds = self.coco.getAnnIds(imgIds = image_id, catIds=self.focus_category, areaRng=self.areaRng, iscrowd=self.iscrowd)\n",
    "        anns = self.coco.loadAnns(AnnIds)  \n",
    " \n",
    "        # fix old format issue: convert [xmin, ymin, width, height] in pycocotools to [xmin, ymin, xmax, ymax] format required by model\n",
    "        boxes=[]\n",
    "        masks=[]\n",
    "        areas=[]\n",
    "        image_id_instance=[]\n",
    "        catIds=[]\n",
    "        \n",
    "\n",
    "        print('AnnIds: {}'.format(AnnIds))   \n",
    "        if len(AnnIds) == 1:\n",
    "            print('type of annotation object anns:{}'.format(type(anns)))\n",
    "            print('------------------'+str(type(anns))+'-------------------')\n",
    "            \n",
    "        #if isinstance(AnnIds, int):\n",
    "            #masks, areas, boxes, image_id_instance, catIds = self.mabic(anns, masks, areas, boxes, image_id_instance, catIds)\n",
    "        #elif len(AnnIds) == 1:\n",
    "            #masks, areas, boxes, image_id_instance, catIds = self.mabic(anns, masks, areas, boxes, image_id_instance, catIds)\n",
    "        #else:\n",
    "        for ann in anns: \n",
    "            masks, areas, boxes, image_id_instance, catIds = self.mabic(ann, masks, areas, boxes, image_id_instance, catIds)\n",
    "        \n",
    " \n",
    "        #get image for return\n",
    "        if type(image_id) == np.int64:\n",
    "            image_id = image_id.item()           #convert numpy integer to python native integer for loadImgs\n",
    "        img_obj = self.coco.loadImgs(image_id)[0]\n",
    "        img = Image.open(os.path.join(self.imgs_dir, img_obj['file_name']))\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        \n",
    "        #note: label 0 is reserved for background in the model\n",
    "        print('catlabel: {}; catIds: {}'.format(self.catlabel, catIds))\n",
    "        labels = [ self.catlabel[i] for i in catIds]\n",
    "        \n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)   \n",
    "        iscrowd = torch.zeros(len(anns), dtype=torch.int64) #because we have excluded crowd\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)  #empty box causes error, reshape from 0 to (0,4) for empty box\n",
    "        masks = torch.as_tensor(np.array(masks), dtype=torch.uint8)\n",
    "        image_id_instance = torch.tensor(image_id_instance)\n",
    "        area = torch.as_tensor(areas)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id_instance\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc2758-7b5e-4427-878b-e2cfacd042a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_obj = COCO(annFile).loadAnns(COCO(annFile).getAnnIds(COCO(annFile).getImgIds(536))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5725bba1-7d8b-4ab4-9058-ccdaf19c35b1",
   "metadata": {},
   "source": [
    "# Instance segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b0f16a-cd1c-4200-b192-8766034541cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "      \n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False,  pretrained_backbone=False,)\n",
    "    pretrained_weights = torch.load('maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth', map_location='cpu')\n",
    "    # # copy only backbone weights\n",
    "    for _n, _par in model.state_dict().items():\n",
    "         if 'backbone' in _n:\n",
    "            _par.requires_grad = False\n",
    "            _par.copy_(pretrained_weights[_n])\n",
    "            _par.requires_grad = True\n",
    "\n",
    "    if device == torch.device('cuda'):\n",
    "        model = model.to(device)\n",
    "    \n",
    "    \n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979999b7-2e9d-429a-a901-7aeaedb4f829",
   "metadata": {},
   "source": [
    "## Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f099aff-3fe3-4d31-b7e7-64b286e84f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our dataset and defined transformations\n",
    "import torchvision.transforms as transforms\n",
    "dataset = CocoDataset('/mnt/data/public/coco2017/coco2017', 'train', transforms.Compose([transforms.ToTensor()]))\n",
    "dataset_test = CocoDataset('/mnt/data/public/coco2017/coco2017', 'val', transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a86982-2dff-4902-95e8-b0ff9f47abee",
   "metadata": {},
   "source": [
    "## Instantiate the model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db96022-b5ab-48bb-9137-d8dd002ed816",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fcc92d-e959-4c46-bee6-b36832c05333",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0745aba7-5e70-42b0-bda1-d707039e2aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's train it for 2 epochs\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 1000 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fd61ab-3a28-4613-aa37-626df06bbb57",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556438e7-c5d0-4eee-9aba-34dc1136fddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one image from the test set\n",
    "img, _ = dataset_test[0]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])\n",
    "    \n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb385df-7d6f-46af-a7e2-f837801642f0",
   "metadata": {},
   "source": [
    "Convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in [C, H, W] format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e20b74-6056-42e1-bfe7-dff4ddbb7ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3000fed7-df77-45f5-911a-a5d8d8b12d59",
   "metadata": {},
   "source": [
    "Let's now visualize the top predicted segmentation mask. The masks are predicted as [N, 1, H, W], where N is the number of predictions, and are probability maps between 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d970e19-8a7c-4711-952f-9a3ba9df19c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fda80b-c78b-4719-84ef-a308dba0e9aa",
   "metadata": {},
   "source": [
    "# debug..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e7bab8-d09c-4a6a-bab4-def0381a6f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "anns = [{'segmentation': [[443.87, 249.81, 438.71, 267.35, 438.71, 284.9, 434.58, 290.06, 425.29, 285.94, 432.52, 261.16, 428.39, 260.13, 426.32, 272.52, 424.26, 279.74, 417.03, 279.74, 421.16, 260.13, 409.81, 260.13, 397.42, 275.61, 396.39, 292.13, 385.03, 313.81, 378.84, 328.26, 374.71, 349.94, 376.77, 360.26, 373.68, 372.65, 359.23, 367.48, 361.29, 349.94, 361.29, 328.26, 365.42, 314.84, 370.58, 299.35, 375.74, 289.03, 349.94, 290.06, 344.77, 308.65, 340.65, 327.23, 340.65, 347.87, 336.52, 355.1, 329.29, 356.13, 330.32, 344.77, 332.39, 335.48, 332.39, 320.0, 330.32, 300.39, 334.45, 282.84, 318.97, 253.94, 313.81, 233.29, 313.81, 211.61, 308.65, 188.9, 307.61, 174.45, 284.9, 172.39, 306.58, 158.97, 322.06, 151.74, 339.61, 145.55, 357.16, 150.71, 367.48, 160.0, 371.61, 161.03, 381.94, 152.77, 395.35, 144.52, 409.81, 138.32, 423.23, 134.19, 436.65, 130.06, 451.1, 131.1, 461.42, 137.29, 464.52, 147.61, 464.52, 157.94, 466.58, 168.26, 466.58, 184.77, 463.48, 209.55, 457.29, 220.9, 452.13, 231.23, 445.94, 242.58]], 'area': 21895.6492, 'iscrowd': 0, 'image_id': 99026, 'bbox': [284.9, 130.06, 181.68, 242.59], 'category_id': 20, 'id': 63673}, {'segmentation': [[105.87, 193.33, 178.84, 178.94, 235.37, 176.89, 272.38, 164.55, 267.24, 154.27, 266.21, 138.86, 255.93, 130.63, 285.74, 122.41, 300.13, 115.22, 327.88, 113.16, 339.19, 125.5, 352.55, 137.83, 358.72, 154.27, 339.19, 150.16, 284.71, 172.78, 306.3, 175.86, 315.55, 228.28, 307.32, 243.7, 300.13, 268.36, 289.85, 292.01, 277.52, 319.76, 269.29, 333.12, 268.27, 360.87, 271.35, 375.26, 268.27, 380.4, 260.04, 376.29, 254.9, 371.15, 253.88, 350.59, 259.01, 332.09, 260.04, 313.59, 257.99, 296.12, 245.65, 301.26, 243.6, 314.62, 236.4, 332.09, 236.4, 347.51, 237.43, 358.81, 233.32, 361.9, 227.15, 358.81, 230.24, 342.37, 227.15, 318.73, 200.43, 329.01, 179.87, 336.2, 164.45, 336.2, 156.23, 339.29, 148.01, 357.79, 139.79, 371.15, 124.37, 391.71, 113.06, 415.35, 115.12, 425.62, 96.62, 420.48, 96.62, 377.32, 99.7, 363.95, 76.06, 353.68, 41.11, 327.98, 24.67, 295.09, 23.64, 265.28, 30.84, 242.67, 48.31, 220.06, 84.28, 205.67, 109.98, 195.39]], 'area': 46748.43474999999, 'iscrowd': 0, 'image_id': 99026, 'bbox': [23.64, 113.16, 335.08, 312.46], 'category_id': 20, 'id': 64255}, {'segmentation': [[20.65, 157.94, 131.1, 146.58, 155.87, 134.19, 190.97, 134.19, 222.97, 141.42, 230.19, 145.55, 215.74, 153.81, 230.19, 175.48, 139.35, 184.77, 99.1, 192.0, 108.39, 195.1, 47.48, 222.97, 40.26, 229.16, 26.84, 217.81]], 'area': 10007.109449999998, 'iscrowd': 0, 'image_id': 99026, 'bbox': [20.65, 134.19, 209.54, 94.97], 'category_id': 20, 'id': 66125}, {'segmentation': [[115.61, 132.13, 126.97, 101.16, 166.19, 99.1, 213.68, 116.65], [164.13, 136.26, 242.58, 118.71, 265.29, 115.61, 271.48, 122.84, 263.23, 132.13, 266.32, 144.52, 267.35, 152.77, 260.13, 161.03, 252.9, 155.87, 247.74, 165.16, 231.23, 172.39, 216.77, 152.77, 228.13, 144.52]], 'area': 4435.479550000002, 'iscrowd': 0, 'image_id': 99026, 'bbox': [115.61, 99.1, 155.87, 73.29], 'category_id': 20, 'id': 66283}, {'segmentation': [[513.73, 75.34, 535.71, 73.45, 541.99, 77.22, 543.88, 88.53, 539.48, 94.18, 536.97, 111.14, 540.74, 125.58, 541.99, 149.45, 536.34, 172.06, 525.66, 195.29, 522.52, 217.27, 523.78, 225.44, 523.78, 232.97, 518.76, 233.6, 514.36, 227.95, 515.62, 217.27, 516.87, 205.34, 515.62, 202.2, 507.45, 219.16, 501.17, 234.23, 495.52, 245.54, 491.75, 255.58, 488.61, 263.12, 486.1, 273.17, 483.59, 280.71, 477.93, 283.85, 472.91, 281.96, 471.03, 278.19, 476.05, 265.0, 480.45, 254.33, 480.45, 242.4, 484.21, 227.95, 487.35, 221.67, 484.84, 218.53, 488.61, 205.34, 486.1, 202.2, 477.93, 199.69, 474.17, 198.43, 457.84, 229.21, 454.07, 254.33, 455.32, 264.38, 451.56, 268.77, 448.42, 268.77, 443.39, 263.12, 446.53, 242.4, 459.72, 208.48, 467.89, 165.15, 460.35, 134.37, 474.17, 119.3, 496.15, 109.88, 502.43, 105.48, 506.82, 90.41, 503.68, 87.27, 496.15, 84.13, 504.31, 80.36, 509.96, 77.85, 513.73, 75.34]], 'area': 9881.042499999998, 'iscrowd': 0, 'image_id': 99026, 'bbox': [443.39, 73.45, 100.49, 210.4], 'category_id': 20, 'id': 66737}, {'segmentation': [[203.19, 36.86, 203.4, 32.91, 201.61, 26.17, 201.69, 16.6, 207.89, 16.72, 207.11, 27.33, 206.26, 32.28, 208.05, 34.38, 208.05, 36.81], [212.85, 35.74, 213.48, 26.58, 211.27, 16.37, 218.53, 16.26, 219.48, 23.42, 218.85, 26.16, 217.59, 36.06], [201.5, 13.83, 199.5, 4.46, 199.19, 0.0, 218.03, 0.0, 219.08, 4.57, 218.35, 13.41]], 'area': 470.5772, 'iscrowd': 0, 'image_id': 99026, 'bbox': [199.19, 0.0, 20.29, 36.86], 'category_id': 1, 'id': 208606}, {'segmentation': [[75.23, 47.14, 74.13, 40.92, 79.62, 38.36, 76.69, 25.92, 65.35, 23.72, 69.37, 48.96, 77.06, 47.5], [57.67, 18.24, 62.42, 45.31, 64.25, 43.48, 59.5, 19.7], [55.11, 26.28, 54.74, 41.65, 58.4, 44.21, 56.57, 46.04, 51.45, 47.5, 49.99, 50.06, 61.69, 47.87, 55.84, 26.65], [49.97, 12.36, 46.95, 4.28, 45.26, 4.62, 42.23, 8.66, 42.91, 12.02, 36.51, 7.99, 41.56, 0.0, 76.91, 0.58, 77.92, 10.34, 75.22, 15.73, 77.24, 20.44, 66.13, 20.44, 63.78, 12.7, 50.98, 14.04]], 'area': 881.2295999999999, 'iscrowd': 0, 'image_id': 99026, 'bbox': [36.51, 0.0, 43.11, 50.06], 'category_id': 1, 'id': 209655}, {'segmentation': [[545.45, 99.77, 565.38, 87.68, 586.02, 87.68, 603.1, 95.5, 615.9, 106.89, 620.17, 125.39, 620.89, 145.32, 616.62, 179.48, 598.83, 209.37, 594.56, 230.0, 590.29, 252.07, 579.61, 250.64, 592.42, 194.42, 581.75, 181.61, 573.92, 177.34, 575.34, 197.98, 572.49, 204.39, 567.51, 205.81, 566.8, 201.54, 568.22, 178.77, 551.86, 186.59, 541.89, 210.08, 532.64, 232.14, 522.68, 232.14, 525.53, 221.47, 532.64, 202.96, 536.2, 183.75, 536.2, 170.23, 541.18, 150.3, 544.03, 134.64, 539.05, 109.03]], 'area': 8651.279750000002, 'iscrowd': 0, 'image_id': 99026, 'bbox': [522.68, 87.68, 98.21, 164.39], 'category_id': 20, 'id': 275887}, {'segmentation': [[294.43, 46.68, 297.72, 33.51, 294.43, 18.14, 292.96, 5.7, 295.52, 0.58, 276.5, 0.58, 274.67, 6.07, 275.77, 14.85, 273.57, 35.33, 274.67, 40.82, 267.72, 45.21, 276.13, 48.5, 280.89, 49.6, 284.55, 28.38, 287.11, 29.12, 284.91, 38.63, 286.01, 50.33, 293.69, 49.97]], 'area': 958.73165, 'iscrowd': 0, 'image_id': 99026, 'bbox': [267.72, 0.58, 30.0, 49.75], 'category_id': 1, 'id': 1229705}, {'segmentation': [[66.42, 139.07, 30.56, 134.31, 17.63, 132.15, 14.68, 136.87, 22.89, 144.22, 36.28, 148.1, 46.56, 146.44], [147.94, 133.72, 44.58, 154.0, 111.37, 146.84, 131.24, 146.05, 148.74, 137.7]], 'area': 900.1238000000003, 'iscrowd': 0, 'image_id': 99026, 'bbox': [14.68, 132.15, 134.06, 21.85], 'category_id': 20, 'id': 1406046}, {'segmentation': [[241.13, 35.86, 241.13, 26.65, 241.13, 21.94, 241.13, 18.57, 240.9, 15.2, 240.45, 12.06, 240.45, 8.24, 239.56, 5.32, 236.19, 0.61, 258.19, 0.38, 259.09, 1.73, 258.87, 3.75, 256.62, 8.02, 255.27, 12.28, 255.27, 17.22, 256.17, 34.74, 248.76, 34.51, 249.66, 18.79, 247.86, 17.45, 245.84, 35.19]], 'area': 504.05975000000063, 'iscrowd': 0, 'image_id': 99026, 'bbox': [236.19, 0.38, 22.9, 35.48], 'category_id': 1, 'id': 1743445}]\n",
    "for idx, ann in enumerate(anns):\n",
    "    print(idx)\n",
    "    print(ann)\n",
    "    print(coco_train.annToMask(ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f1fb1-ff32-43de-9b3d-34d5a41106db",
   "metadata": {},
   "outputs": [],
   "source": [
    "AnnIds = [63673, 64255, 66125, 66283, 66737, 208606, 209655, 275887, 1229705, 1406046, 1743445]\n",
    "if isinstance(AnnIds, int):\n",
    "    print('int')\n",
    "elif len(AnnIds) == 1:\n",
    "    print(1)\n",
    "else:\n",
    "    print('else')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae6f601-4b08-4a35-9782-30d1516e1863",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes=[]\n",
    "masks=[]\n",
    "areas=[]\n",
    "image_id_instance=[]\n",
    "for ann in anns:\n",
    "    print(type(ann['image_id']))\n",
    "    masks.append(coco_train.annToMask(ann))\n",
    "    areas.append(ann['area'])\n",
    "    boxes.append([ann['bbox'][0], ann['bbox'][1], ann['bbox'][0]+ann['bbox'][2]+1e-7, ann['bbox'][1]+ann['bbox'][3]+1e-7]) #1e-7 solves float num rounding issue\n",
    "    image_id_instance.append(ann['image_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766f3d85-6b8d-40b3-bfd3-2636b09bdc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ann in {'segmentation': [[241.13, 35.86, 241.13, 26.65, 241.13, 21.94, 241.13, 18.57, 240.9, 15.2, 240.45, 12.06, 240.45, 8.24, 239.56, 5.32, 236.19, 0.61, 258.19, 0.38, 259.09, 1.73, 258.87, 3.75, 256.62, 8.02, 255.27, 12.28, 255.27, 17.22, 256.17, 34.74, 248.76, 34.51, 249.66, 18.79, 247.86, 17.45, 245.84, 35.19]], 'area': 504.05975000000063, 'iscrowd': 0, 'image_id': 99026, 'bbox': [236.19, 0.38, 22.9, 35.48], 'category_id': 1, 'id': 1743445}:\n",
    "    print(ann)\n",
    "    coco_train.annToMask(ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ec5a2-0184-4536-bf34-4f7320a07fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_train.annToMask('segmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8bace9-b174-4f69-ab69-be0cacadcee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_train.imgs[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95f65b9-1a33-47b8-be13-728bda4bf535",
   "metadata": {},
   "outputs": [],
   "source": [
    "last = {'segmentation': [[241.13, 35.86, 241.13, 26.65, 241.13, 21.94, 241.13, 18.57, 240.9, 15.2, 240.45, 12.06, 240.45, 8.24, 239.56, 5.32, 236.19, 0.61, 258.19, 0.38, 259.09, 1.73, 258.87, 3.75, 256.62, 8.02, 255.27, 12.28, 255.27, 17.22, 256.17, 34.74, 248.76, 34.51, 249.66, 18.79, 247.86, 17.45, 245.84, 35.19]], 'area': 504.05975000000063, 'iscrowd': 0, 'image_id': 99026, 'bbox': [236.19, 0.38, 22.9, 35.48], 'category_id': 1, 'id': 1743445}\n",
    "last['image_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e9d356-14f9-49c3-bf9b-5c4c7d5243bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_train.imgs[99062]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2bf5fd-540a-4793-8247-ef5c7ebc12e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(coco_train.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763cb38d-0de6-434d-a2b2-16796d5bb80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6383ad7-0b5d-4486-9029-2ac32e7b390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9537c98f-7f10-4765-aa09-7e4fb4ccf340",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([37909, 50503, 126550, 1898727])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7438226b-9b49-42b3-a526-ba400cbba72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "anns = coco_train.loadAnns([37909, 50503, 126550, 1898727])\n",
    "ann=anns[-1]\n",
    "coco_train.annToMask(ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8437e23-0919-4ebc-8947-dd314de2431e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
